GPT-2 Output Detector
=====================

This directory (`/detector`) contains the code for working with the GPT-2 output detector model, obtained by fine-tuning a
[RoBERTa model](https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/)
with [the outputs of the 1.5B-parameter GPT-2 model](https://github.com/openai/gpt-2-output-dataset).
For motivations and discussions regarding the release of this detector model, please check out 
[our blog post](https://openai.com/blog/gpt-2-1-5b-release/) and [report](https://d4mucfpksywv.cloudfront.net/papers/GPT_2_Report.pdf).

## Downloading a pre-trained detector model

Download the weights for the fine-tuned `roberta-base` model (478 MB):

```bash
wget https://storage.googleapis.com/gpt-2/detector-models/v1/detector-base.pt
```

or `roberta-large` model (1.5 GB):

```bash
wget https://storage.googleapis.com/gpt-2/detector-models/v1/detector-large.pt
```

These RoBERTa-based models are fine-tuned with a mixture of temperature-1 and nucleus sampling outputs,
which should generalize well to outputs generated using different sampling methods.

## Running a detector model

You can launch a web UI in which you can enter a text and see the detector model's prediction
on whether or not it was generated by a GPT-2 model.

```bash
# (on the top-level directory of this repository)
pip install -r requirements.txt
python -m detector.server detector-base.pt
```

After the script says "Ready to serve", nagivate to http://localhost:8080 to view the UI.

## Training a new detector model

You can use the provided training script to train a detector model on a new set of datasets.
We recommend using a GPU machine for this task.

```bash
# (on the top-level directory of this repository)
pip install -r requirements.txt
python -m detector.train
```

The training script supports a number of different options; append `--help` to the command above for usage.

___

# gpt-2-output-dataset

This dataset contains:
- 250K documents from the WebText test set
- For each GPT-2 model (trained on the WebText training set), 250K random samples (temperature 1, no truncation) and 250K samples generated with Top-K 40 truncation

We look forward to the research produced using this data!

### Download

For each model, we have a training split of 250K generated examples, as well as validation and test splits of 5K examples.

All data is located in Google Cloud Storage, under the directory `gs://gpt-2/output-dataset/v1`.

There, you will find files:

- `webtext.${split}.jsonl`
- `small-117M.${split}.jsonl`
- `small-117M-k40.${split}.jsonl`
- `medium-345M.${split}.jsonl`
- `medium-345M-k40.${split}.jsonl`
- `large-762M.${split}.jsonl`
- `large-762M-k40.${split}.jsonl`
- `xl-1542M.${split}.jsonl`
- `xl-1542M-k40.${split}.jsonl`

where split is one of `train`, `test`, and `valid`.

We've provided a script to download all of them, in `download_dataset.py`.

#### Finetuned model samples

Additionally, we encourage research on detection of finetuned models.  We have released data under `gs://gpt-2/output-dataset/v1-amazonfinetune/` with samples from a GPT-2 full model finetuned to output Amazon reviews.

### Detectability baselines

We're interested in seeing research in detectability of GPT-2 model family generations.

We provide some [initial analysis](detection.md) of two baselines, as well as [code](./baseline.py) for the better baseline.

Overall, we are able to achieve accuracies in the mid-90s for Top-K 40 generations, and mid-70s to high-80s (depending on model size) for random generations.  We also find some evidence that adversaries can evade detection via finetuning from released models.

### Data removal requests

If you believe your work is included in WebText and would like us to remove it, please let us know at webtextdata@openai.com.
